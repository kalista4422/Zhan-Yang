# geom_prompt_extractor/sam_core.py
"""
Part 1: SAMæ¨¡å‹æ ¸å¿ƒç®¡ç†ä¸æ£€æµ‹åŠŸèƒ½ - ç¨³å®šä¼˜åŒ–ç‰ˆ
åŒ…å«ï¼šSAMæ¨¡å‹åŠ è½½ã€å¢å¼ºå†…å®¹æ¡†æ£€æµ‹ã€å¯¹è±¡ç±»å‹æ£€æµ‹ã€åˆå§‹æ©ç ç”Ÿæˆ
é‡ç‚¹ä¼˜åŒ–ï¼šç§»é™¤æœ‰é—®é¢˜çš„å‡ ä½•çº¦æŸï¼Œä¸“æ³¨äºç¨³å®šå¯é çš„å¤šç­–ç•¥æ£€æµ‹
"""

import torch
from PIL import Image
import numpy as np
import cv2
import os
import sys
from typing import Tuple, List, Dict, Optional
import scipy.ndimage as ndimage
from pathlib import Path
from sklearn.cluster import KMeans

# ä»é…ç½®æ¨¡å—å¯¼å…¥
from .config import ExtractionConfig, ObjectType

# å¯¼å…¥åŸå§‹SAMåº“
try:
    from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator
except ImportError:
    print("é”™è¯¯ï¼šæ— æ³•å¯¼å…¥ 'segment_anything'ã€‚è¯·ç¡®ä¿å·²å®‰è£…ã€‚")
    print("å®‰è£…å‘½ä»¤: pip install git+https://github.com/facebookresearch/segment-anything.git ")
    sys.exit(1)

# æ£€æŸ¥æ˜¯å¦æœ‰ximgprocæ¨¡å—
try:
    import cv2.ximgproc
    HAS_XIMGPROC = True
except:
    HAS_XIMGPROC = False


class SAMCore:
    """SAMæ ¸å¿ƒåŠŸèƒ½ç±» - æ¨¡å‹ç®¡ç†ã€æ£€æµ‹å’Œåˆå§‹åˆ†å‰²"""

    def __init__(self, checkpoint_path: str, model_type: str = 'vit_h',
                 device: str = None, force_gpu: bool = True):
        """åˆå§‹åŒ–SAMæ¨¡å‹å’Œé…ç½®"""
        # è®¾å¤‡é…ç½®
        if force_gpu and torch.cuda.is_available():
            self.device = "cuda"
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.enabled = True
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
            print(f"ğŸš€ ä½¿ç”¨GPU: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            self.device = device or "cpu"
            print(f"âš ï¸ ä½¿ç”¨CPUæ¨¡å¼")

        # åŠ è½½SAMæ¨¡å‹
        print(f"ğŸ“¦ åŠ è½½SAMæ¨¡å‹ ({model_type})...")
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")

        self.sam = sam_model_registry[model_type](checkpoint=checkpoint_path)
        self.sam.to(device=self.device)
        self.sam.eval()
        self.predictor = SamPredictor(self.sam)
        self.config = ExtractionConfig()

        print(f"âœ… SAMæ¨¡å‹åŠ è½½å®Œæˆ!")

    # ============= æ”¹è¿›çš„å¯¹è±¡ç±»å‹æ£€æµ‹ =============

    def detect_object_type(self, image: Image.Image) -> ObjectType:
        """æ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡ç±»å‹ - æ”¹è¿›ç‰ˆ"""
        img_array = np.array(image)
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)

        # åŸºç¡€ç‰¹å¾è®¡ç®—
        s_mean = hsv[:, :, 1].mean()
        v_std = hsv[:, :, 2].std()
        brightness_mean = hsv[:, :, 2].mean()

        # è¾¹ç¼˜å’Œçº¹ç†ç‰¹å¾
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size

        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        texture_complexity = laplacian.var()

        # å¯¹æ¯”åº¦ç‰¹å¾
        contrast = gray.std()

        # é¢œè‰²åˆ†å¸ƒç‰¹å¾
        color_hist = cv2.calcHist([img_array], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
        color_entropy = -np.sum(color_hist * np.log(color_hist + 1e-10))

        # æ”¹è¿›çš„åˆ†ç±»é€»è¾‘
        # 1. é›•åˆ»å“ç‰¹å¾ï¼šé«˜è¾¹ç¼˜å¯†åº¦ + é«˜çº¹ç†å¤æ‚åº¦ + ä¸­ç­‰å¯¹æ¯”åº¦
        if (edge_density > 0.12 and texture_complexity > 600 and
                30 < contrast < 80 and color_entropy > 5000):
            return ObjectType.CARVING

        # 2. æµ®é›•ç‰¹å¾ï¼šä½é¥±å’Œåº¦ + ä¸­ç­‰çº¹ç† + ç‰¹å®šäº®åº¦èŒƒå›´
        elif (s_mean < 35 and texture_complexity > 300 and
              80 < brightness_mean < 200 and edge_density > 0.04):
            return ObjectType.RELIEF

        # 3. èŠ±è‰ç‰¹å¾ï¼šé«˜é¥±å’Œåº¦ + é«˜é¢œè‰²ç†µ + ä¸­ç­‰è¾¹ç¼˜å¯†åº¦
        elif (s_mean > 50 and color_entropy > 6000 and
              0.06 < edge_density < 0.15 and v_std > 25):
            return ObjectType.FLOWER

        # 4. äº§å“ç‰¹å¾ï¼šä½é¥±å’Œåº¦ + ä½æ–¹å·® + å‡åŒ€åˆ†å¸ƒ
        elif (s_mean < 40 and v_std < 30 and texture_complexity < 400):
            return ObjectType.PRODUCT

        # 5. é»˜è®¤ç±»å‹
        else:
            return ObjectType.GENERAL

    # ============= ç¨³å®šçš„å¤šç­–ç•¥å†…å®¹æ¡†æ£€æµ‹ =============

    def detect_content_box(self, image: Image.Image,
                           object_type: ObjectType) -> Tuple[int, int, int, int]:
        """ç¨³å®šçš„å¤šç­–ç•¥å†…å®¹æ¡†æ£€æµ‹ - ç§»é™¤é—®é¢˜å‡ ä½•çº¦æŸ"""
        img_array = np.array(image)
        h, w = img_array.shape[:2]

        print(f"  ğŸ” æ‰§è¡Œå¢å¼ºå¤šç­–ç•¥å†…å®¹æ¡†æ£€æµ‹...")

        # ç­–ç•¥1: æ”¹è¿›çš„æ˜¾è‘—æ€§æ£€æµ‹
        saliency_box = self._enhanced_saliency_detection(img_array, object_type)

        # ç­–ç•¥2: è‡ªé€‚åº”GrabCutæ£€æµ‹
        grabcut_box = self._adaptive_grabcut_detection(img_array, object_type)

        # ç­–ç•¥3: åˆ†å±‚è¾¹ç¼˜æ£€æµ‹
        edge_box = self._hierarchical_edge_detection(img_array, object_type)

        # ç­–ç•¥4: æ™ºèƒ½é¢œè‰²èšç±»æ£€æµ‹
        color_box = self._intelligent_color_clustering(img_array, object_type)

        # ç­–ç•¥5: æ··åˆé˜ˆå€¼æ£€æµ‹
        threshold_box = self._hybrid_threshold_detection(img_array, object_type)

        # ç­–ç•¥6: è½®å»“åˆ†ææ£€æµ‹ï¼ˆæ–°å¢ï¼‰
        contour_box = self._contour_analysis_detection(img_array, object_type)

        # æ”¶é›†å€™é€‰æ¡†
        candidate_boxes = []
        for box, name in [
            (saliency_box, "å¢å¼ºæ˜¾è‘—æ€§"),
            (grabcut_box, "è‡ªé€‚åº”GrabCut"),
            (edge_box, "åˆ†å±‚è¾¹ç¼˜"),
            (color_box, "æ™ºèƒ½èšç±»"),
            (threshold_box, "æ··åˆé˜ˆå€¼"),
            (contour_box, "è½®å»“åˆ†æ")
        ]:
            if box and self._validate_box(box, (h, w)):
                score = self._comprehensive_box_scoring(box, img_array, object_type)
                candidate_boxes.append((box, score, name))
                print(f"    âœ“ {name}: ç½®ä¿¡åº¦ {score:.3f}")

        if not candidate_boxes:
            # æ™ºèƒ½é™çº§ç­–ç•¥
            return self._intelligent_fallback_box(img_array, object_type)

        # æ™ºèƒ½æ¡†é€‰æ‹©å’Œèåˆ
        best_box = self._intelligent_box_selection(candidate_boxes, img_array, object_type)

        # è‡ªé€‚åº”è¾¹ç•Œç»†åŒ–
        final_box = self._adaptive_box_refinement(best_box, img_array, object_type)

        return final_box

    def _enhanced_saliency_detection(self, img_array: np.ndarray,
                                     object_type: ObjectType) -> Optional[Tuple[int, int, int, int]]:
        """å¢å¼ºçš„æ˜¾è‘—æ€§æ£€æµ‹"""
        try:
            # å¤šå°ºåº¦æ˜¾è‘—æ€§æ£€æµ‹
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)

            # æ–¹æ³•1: æ‹‰æ™®æ‹‰æ–¯æ˜¾è‘—æ€§
            laplacian = cv2.Laplacian(gray, cv2.CV_64F)
            laplacian_abs = np.abs(laplacian)

            # æ–¹æ³•2: é¢‘åŸŸæ˜¾è‘—æ€§
            f_transform = np.fft.fft2(gray)
            f_shift = np.fft.fftshift(f_transform)
            magnitude = np.abs(f_shift)
            phase = np.angle(f_shift)

            # é‡æ„æ˜¾è‘—æ€§å›¾
            log_magnitude = np.log(magnitude + 1)
            residual = log_magnitude - cv2.GaussianBlur(log_magnitude, (3, 3), 0)

            # ç»“åˆå¤šç§æ˜¾è‘—æ€§
            saliency_map = laplacian_abs * 0.4 + np.abs(residual) * 0.6

            # æ ¹æ®å¯¹è±¡ç±»å‹è°ƒæ•´é˜ˆå€¼
            if object_type == ObjectType.CARVING:
                threshold = np.percentile(saliency_map, 70)
            elif object_type == ObjectType.RELIEF:
                threshold = np.percentile(saliency_map, 60)
            else:
                threshold = np.percentile(saliency_map, 75)

            binary = (saliency_map > threshold).astype(np.uint8) * 255

            # å½¢æ€å­¦æ¸…ç†
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
            binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)

            return self._find_optimal_bounding_box(binary)

        except Exception:
            return None

    def _adaptive_grabcut_detection(self, img_array: np.ndarray,
                                    object_type: ObjectType) -> Optional[Tuple[int, int, int, int]]:
        """è‡ªé€‚åº”GrabCutæ£€æµ‹"""
        try:
            h, w = img_array.shape[:2]

            # æ ¹æ®å¯¹è±¡ç±»å‹è°ƒæ•´åˆå§‹çŸ©å½¢
            if object_type == ObjectType.CARVING:
                margin_ratio = 0.15
            elif object_type == ObjectType.RELIEF:
                margin_ratio = 0.12
            else:
                margin_ratio = 0.18

            rect = (int(w * margin_ratio), int(h * margin_ratio),
                    int(w * (1 - 2 * margin_ratio)), int(h * (1 - 2 * margin_ratio)))

            # å¤šæ¬¡è¿­ä»£GrabCut
            mask = np.zeros((h, w), np.uint8)
            bgd_model = np.zeros((1, 65), np.float64)
            fgd_model = np.zeros((1, 65), np.float64)

            # ç¬¬ä¸€æ¬¡è¿­ä»£
            cv2.grabCut(img_array, mask, rect, bgd_model, fgd_model, 3, cv2.GC_INIT_WITH_RECT)

            # æå–å‰æ™¯
            mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')

            # å¦‚æœç»“æœå¤ªå°ï¼Œå†æ¬¡å°è¯•
            if np.sum(mask2) < (w * h * 0.05):
                rect = (int(w * 0.05), int(h * 0.05),
                        int(w * 0.9), int(h * 0.9))
                mask = np.zeros((h, w), np.uint8)
                cv2.grabCut(img_array, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)
                mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')

            return self._find_optimal_bounding_box(mask2 * 255)

        except Exception:
            return None

    def _hierarchical_edge_detection(self, img_array: np.ndarray,
                                     object_type: ObjectType) -> Optional[Tuple[int, int, int, int]]:
        """åˆ†å±‚è¾¹ç¼˜æ£€æµ‹"""
        try:
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)

            # é¢„å¤„ç†
            if object_type == ObjectType.RELIEF:
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                gray = clahe.apply(gray)
            elif object_type == ObjectType.CARVING:
                gray = cv2.bilateralFilter(gray, 9, 75, 75)

            # å¤šå°ºåº¦è¾¹ç¼˜æ£€æµ‹
            edges_pyramid = []
            for scale in [0.5, 1.0, 1.5]:
                scaled = cv2.resize(gray, None, fx=scale, fy=scale)

                if object_type == ObjectType.CARVING:
                    edges = cv2.Canny(scaled, 30, 90)
                elif object_type == ObjectType.RELIEF:
                    edges = cv2.Canny(scaled, 20, 60)
                else:
                    edges = cv2.Canny(scaled, 50, 150)

                edges = cv2.resize(edges, (img_array.shape[1], img_array.shape[0]))
                edges_pyramid.append(edges)

            # èåˆå¤šå°ºåº¦è¾¹ç¼˜
            combined_edges = np.zeros_like(edges_pyramid[0])
            weights = [0.3, 0.5, 0.2]
            for i, edges in enumerate(edges_pyramid):
                combined_edges += edges * weights[i]

            combined_edges = (combined_edges > 128).astype(np.uint8) * 255

            # å½¢æ€å­¦å¤„ç†
            kernel_close = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
            combined_edges = cv2.morphologyEx(combined_edges, cv2.MORPH_CLOSE, kernel_close, iterations=2)

            kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
            combined_edges = cv2.morphologyEx(combined_edges, cv2.MORPH_OPEN, kernel_open)

            return self._find_optimal_bounding_box(combined_edges)

        except Exception:
            return None

    def _intelligent_color_clustering(self, img_array: np.ndarray,
                                      object_type: ObjectType) -> Optional[Tuple[int, int, int, int]]:
        """æ™ºèƒ½é¢œè‰²èšç±»æ£€æµ‹"""
        try:
            h, w = img_array.shape[:2]

            # é™é‡‡æ ·ä»¥æé«˜é€Ÿåº¦
            scale = 0.3 if min(h, w) > 1000 else 0.5
            small_img = cv2.resize(img_array, None, fx=scale, fy=scale)

            # è½¬æ¢åˆ°æœ€é€‚åˆçš„é¢œè‰²ç©ºé—´
            if object_type in [ObjectType.CARVING, ObjectType.RELIEF]:
                color_img = cv2.cvtColor(small_img, cv2.COLOR_RGB2LAB)
            else:
                color_img = cv2.cvtColor(small_img, cv2.COLOR_RGB2HSV)

            # åŠ¨æ€ç¡®å®šèšç±»æ•°é‡
            if object_type == ObjectType.RELIEF:
                n_clusters = 3
            elif object_type == ObjectType.CARVING:
                n_clusters = 4
            else:
                n_clusters = 5

            # K-meansèšç±»
            pixels = color_img.reshape(-1, 3)
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            labels = kmeans.fit_predict(pixels)

            # é‡æ„æ ‡ç­¾å›¾
            label_img = labels.reshape(small_img.shape[:2])

            # æ™ºèƒ½å‰æ™¯è¯†åˆ«
            # æ–¹æ³•1: ä¸­å¿ƒåŒºåŸŸåˆ†æ
            center_h, center_w = label_img.shape[0] // 2, label_img.shape[1] // 2
            center_region = label_img[center_h - center_h // 3:center_h + center_h // 3,
                            center_w - center_w // 3:center_w + center_w // 3]

            # æ–¹æ³•2: åŒºåŸŸå¤§å°åˆ†æ
            unique_labels, counts = np.unique(label_img, return_counts=True)

            # ç»¼åˆè¯„åˆ†é€‰æ‹©å‰æ™¯
            best_score = -1
            best_label = 0

            for label, count in zip(unique_labels, counts):
                # ä¸­å¿ƒåŒºåŸŸå‡ºç°é¢‘ç‡
                center_freq = np.sum(center_region == label) / center_region.size
                # åŒºåŸŸå¤§å°é€‚ä¸­æ€§ï¼ˆé¿å…å¤ªå¤§æˆ–å¤ªå°çš„åŒºåŸŸï¼‰
                size_score = 1 - abs(count / label_img.size - 0.4) * 2
                size_score = max(0, size_score)

                # ç»¼åˆè¯„åˆ†
                total_score = center_freq * 0.6 + size_score * 0.4

                if total_score > best_score:
                    best_score = total_score
                    best_label = label

            # åˆ›å»ºå‰æ™¯æ©ç 
            fg_mask = (label_img == best_label).astype(np.uint8) * 255

            # æ”¾å¤§å›åŸå§‹å°ºå¯¸
            fg_mask = cv2.resize(fg_mask, (w, h), interpolation=cv2.INTER_NEAREST)

            # å½¢æ€å­¦ä¼˜åŒ–
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
            fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel, iterations=2)
            fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel, iterations=1)

            return self._find_optimal_bounding_box(fg_mask)

        except Exception:
            return None

    def _hybrid_threshold_detection(self, img_array: np.ndarray,
                                    object_type: ObjectType) -> Optional[Tuple[int, int, int, int]]:
        """æ··åˆé˜ˆå€¼æ£€æµ‹"""
        try:
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)

            # æ ¹æ®å¯¹è±¡ç±»å‹é€‰æ‹©å‚æ•°
            if object_type == ObjectType.RELIEF:
                block_size = 61
                C1, C2 = 3, 5
            elif object_type == ObjectType.CARVING:
                block_size = 41
                C1, C2 = 8, 12
            else:
                block_size = 31
                C1, C2 = 5, 10

            # å¤šç§è‡ªé€‚åº”é˜ˆå€¼
            binary1 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                            cv2.THRESH_BINARY_INV, block_size, C1)
            binary2 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
                                            cv2.THRESH_BINARY_INV, block_size, C2)

            # Otsué˜ˆå€¼
            _, binary3 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

            # æ™ºèƒ½èåˆ
            if object_type == ObjectType.CARVING:
                # é›•åˆ»å“å¼ºè°ƒçº¹ç†ç»†èŠ‚
                combined = cv2.bitwise_and(binary1, binary2)
                combined = cv2.bitwise_or(combined, binary3)
            else:
                # å…¶ä»–ç±»å‹æ›´ä¿å®ˆ
                combined = cv2.bitwise_and(binary1, binary2)

            # å™ªå£°å»é™¤
            combined = cv2.medianBlur(combined, 5)

            # å½¢æ€å­¦å¤„ç†
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            combined = cv2.morphologyEx(combined, cv2.MORPH_CLOSE, kernel, iterations=2)
            combined = cv2.morphologyEx(combined, cv2.MORPH_OPEN, kernel, iterations=1)

            return self._find_optimal_bounding_box(combined)

        except Exception:
            return None

    def _contour_analysis_detection(self, img_array: np.ndarray,
                                    object_type: ObjectType) -> Optional[Tuple[int, int, int, int]]:
        """è½®å»“åˆ†ææ£€æµ‹ - æ–°å¢æ–¹æ³•"""
        try:
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)

            # è¾¹ç¼˜æ£€æµ‹
            if object_type == ObjectType.CARVING:
                edges = cv2.Canny(gray, 30, 100)
            elif object_type == ObjectType.RELIEF:
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                gray = clahe.apply(gray)
                edges = cv2.Canny(gray, 20, 80)
            else:
                edges = cv2.Canny(gray, 50, 150)

            # æŸ¥æ‰¾è½®å»“
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            if not contours:
                return None

            # è½®å»“ç­›é€‰å’Œè¯„åˆ†
            h, w = img_array.shape[:2]
            valid_contours = []

            for contour in contours:
                area = cv2.contourArea(contour)
                area_ratio = area / (w * h)

                # åŸºæœ¬é¢ç§¯ç­›é€‰
                if area_ratio < 0.01 or area_ratio > 0.8:
                    continue

                # è½®å»“å¤æ‚åº¦
                perimeter = cv2.arcLength(contour, True)
                if perimeter == 0:
                    continue

                # ç´§å‡‘æ€§ï¼ˆé¢ç§¯å‘¨é•¿æ¯”ï¼‰
                compactness = 4 * np.pi * area / (perimeter * perimeter)

                # ä½ç½®è¯„åˆ†ï¼ˆè·ç¦»ä¸­å¿ƒçš„è¿œè¿‘ï¼‰
                M = cv2.moments(contour)
                if M["m00"] != 0:
                    cx = int(M["m10"] / M["m00"])
                    cy = int(M["m01"] / M["m00"])
                    center_distance = np.sqrt((cx - w / 2) ** 2 + (cy - h / 2) ** 2)
                    max_distance = np.sqrt((w / 2) ** 2 + (h / 2) ** 2)
                    position_score = 1 - center_distance / max_distance
                else:
                    position_score = 0

                # ç»¼åˆè¯„åˆ†
                total_score = area_ratio * 0.4 + compactness * 0.3 + position_score * 0.3

                valid_contours.append((contour, total_score))

            if not valid_contours:
                return None

            # é€‰æ‹©æœ€ä½³è½®å»“
            best_contour = max(valid_contours, key=lambda x: x[1])[0]

            # è·å–è¾¹ç•Œæ¡†
            x, y, w, h = cv2.boundingRect(best_contour)
            return (x, y, w, h)

        except Exception:
            return None

    def _find_optimal_bounding_box(self, binary_mask: np.ndarray) -> Optional[Tuple[int, int, int, int]]:
        """ä»äºŒå€¼æ©ç ä¸­æ‰¾åˆ°æœ€ä¼˜è¾¹ç•Œæ¡†"""
        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        if not contours:
            return None

        h, w = binary_mask.shape[:2]
        min_area = w * h * 0.02  # æœ€å°2%
        max_area = w * h * 0.9  # æœ€å¤§90%

        valid_boxes = []

        for contour in sorted(contours, key=cv2.contourArea, reverse=True)[:5]:
            area = cv2.contourArea(contour)
            if min_area < area < max_area:
                # ä½¿ç”¨æœ€å°å¤–æ¥çŸ©å½¢
                x, y, cw, ch = cv2.boundingRect(contour)

                # éªŒè¯å®½é«˜æ¯”
                aspect_ratio = cw / ch if ch > 0 else 0
                if 0.1 < aspect_ratio < 10.0:
                    # è®¡ç®—å¡«å……ç‡
                    fill_ratio = area / (cw * ch) if (cw * ch) > 0 else 0

                    # ç»¼åˆè¯„åˆ†ï¼šé¢ç§¯ + å¡«å……ç‡ + ä½ç½®
                    center_x, center_y = x + cw / 2, y + ch / 2
                    center_distance = np.sqrt((center_x - w / 2) ** 2 + (center_y - h / 2) ** 2)
                    max_distance = np.sqrt((w / 2) ** 2 + (h / 2) ** 2)
                    position_score = 1 - center_distance / max_distance

                    total_score = area * 0.5 + fill_ratio * 0.3 + position_score * 0.2

                    valid_boxes.append(((x, y, cw, ch), total_score))

        if valid_boxes:
            return max(valid_boxes, key=lambda x: x[1])[0]

        return None

    def _validate_box(self, box: Tuple[int, int, int, int],
                      shape: Tuple[int, int]) -> bool:
        """éªŒè¯æ¡†çš„æœ‰æ•ˆæ€§"""
        if box is None:
            return False

        x, y, w, h = box
        img_h, img_w = shape

        # è¾¹ç•Œæ£€æŸ¥
        if x < 0 or y < 0 or x + w > img_w or y + h > img_h:
            return False

        # å°ºå¯¸æ£€æŸ¥
        if w < 20 or h < 20:  # æœ€å°åƒç´ è¦æ±‚
            return False

        area_ratio = (w * h) / (img_w * img_h)
        if area_ratio < 0.01 or area_ratio > 0.95:
            return False

        # å®½é«˜æ¯”æ£€æŸ¥
        aspect_ratio = w / h if h > 0 else 0
        if aspect_ratio < 0.1 or aspect_ratio > 10.0:
            return False

        return True

    def _comprehensive_box_scoring(self, box: Tuple[int, int, int, int],
                                   img_array: np.ndarray,
                                   object_type: ObjectType) -> float:
        """ç»¼åˆæ¡†è´¨é‡è¯„åˆ†"""
        x, y, w, h = box
        roi = img_array[y:y + h, x:x + w]
        img_h, img_w = img_array.shape[:2]

        score = 0.0

        # 1. ä½ç½®å¾—åˆ† (15%)
        center_x, center_y = x + w / 2, y + h / 2
        dist_to_center = np.sqrt((center_x - img_w / 2) ** 2 + (center_y - img_h / 2) ** 2)
        max_dist = np.sqrt((img_w / 2) ** 2 + (img_h / 2) ** 2)
        position_score = 1 - dist_to_center / max_dist
        score += position_score * 0.15

        # 2. å°ºå¯¸å¾—åˆ† (20%)
        area_ratio = (w * h) / (img_w * img_h)
        if 0.1 < area_ratio < 0.7:
            size_score = 1.0
        elif 0.05 < area_ratio < 0.85:
            size_score = 0.8
        else:
            size_score = 0.4
        score += size_score * 0.20

        # 3. å†…å®¹å¤æ‚åº¦å¾—åˆ† (25%)
        gray_roi = cv2.cvtColor(roi, cv2.COLOR_RGB2GRAY)

        # è¾¹ç¼˜å¯†åº¦
        edges = cv2.Canny(gray_roi, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size

        # çº¹ç†å¤æ‚åº¦
        laplacian = cv2.Laplacian(gray_roi, cv2.CV_64F)
        texture_variance = laplacian.var()

        # æ ¹æ®å¯¹è±¡ç±»å‹è°ƒæ•´æœŸæœ›å€¼
        if object_type == ObjectType.CARVING:
            expected_edge_density = 0.15
            expected_texture = 800
        elif object_type == ObjectType.RELIEF:
            expected_edge_density = 0.08
            expected_texture = 400
        elif object_type == ObjectType.FLOWER:
            expected_edge_density = 0.12
            expected_texture = 600
        else:
            expected_edge_density = 0.10
            expected_texture = 500

        edge_score = 1 - abs(edge_density - expected_edge_density) * 3
        edge_score = max(0, min(1, edge_score))

        texture_score = min(texture_variance / expected_texture, 1.0)

        content_score = (edge_score + texture_score) / 2
        score += content_score * 0.25

        # 4. å¯¹æ¯”åº¦å¾—åˆ† (20%)
        contrast = gray_roi.std()
        contrast_score = min(contrast / 60, 1.0)
        score += contrast_score * 0.20

        # 5. é¢œè‰²ä¸€è‡´æ€§å¾—åˆ† (20%)
        if roi.shape[2] == 3:
            hsv_roi = cv2.cvtColor(roi, cv2.COLOR_RGB2HSV)

            # è‰²è°ƒæ ‡å‡†å·®ï¼ˆå€¼è¶Šå°è¶Šä¸€è‡´ï¼‰
            h_std = hsv_roi[:, :, 0].std()
            hue_consistency = max(0, 1 - h_std / 60)

            # é¥±å’Œåº¦åˆ†å¸ƒ
            s_mean = hsv_roi[:, :, 1].mean()
            saturation_score = s_mean / 255.0

            color_score = (hue_consistency + saturation_score) / 2
        else:
            color_score = 0.5

        score += color_score * 0.20

        return min(score, 1.0)

    def _intelligent_box_selection(self, candidate_boxes: List[Tuple[Tuple[int, int, int, int], float, str]],
                                   img_array: np.ndarray,
                                   object_type: ObjectType) -> Tuple[int, int, int, int]:
        """æ™ºèƒ½æ¡†é€‰æ‹©å’Œèåˆ"""
        if len(candidate_boxes) == 1:
            return candidate_boxes[0][0]

        # æŒ‰åˆ†æ•°æ’åº
        candidate_boxes.sort(key=lambda x: x[1], reverse=True)

        # æ ¹æ®å¯¹è±¡ç±»å‹è°ƒæ•´é€‰æ‹©ç­–ç•¥
        if object_type == ObjectType.CARVING:
            confidence_threshold = 0.15
        elif object_type == ObjectType.RELIEF:
            confidence_threshold = 0.25
        else:
            confidence_threshold = 0.20

        # å¦‚æœæœ€é«˜åˆ†æ˜æ˜¾ä¼˜äºå…¶ä»–
        if len(candidate_boxes) > 1 and candidate_boxes[0][1] > candidate_boxes[1][1] + confidence_threshold:
            print(f"  âœ… é€‰æ‹©æœ€ä½³ç­–ç•¥: {candidate_boxes[0][2]} (åˆ†æ•°: {candidate_boxes[0][1]:.3f})")
            return candidate_boxes[0][0]

        # å¦åˆ™è€ƒè™‘èåˆé«˜åˆ†æ¡†
        high_score_threshold = 0.65
        high_score_boxes = [(box, score, name) for box, score, name in candidate_boxes
                            if score > high_score_threshold]

        if len(high_score_boxes) >= 2:
            # åŠ æƒå¹³å‡èåˆ
            boxes = [box for box, _, _ in high_score_boxes]
            weights = [score for _, score, _ in high_score_boxes]
            total_weight = sum(weights)

            if total_weight > 0:
                avg_x = sum(box[0] * w for box, w in zip(boxes, weights)) / total_weight
                avg_y = sum(box[1] * w for box, w in zip(boxes, weights)) / total_weight
                avg_w = sum(box[2] * w for box, w in zip(boxes, weights)) / total_weight
                avg_h = sum(box[3] * w for box, w in zip(boxes, weights)) / total_weight

                print(f"  âœ… èåˆ{len(high_score_boxes)}ä¸ªé«˜åˆ†æ¡†")
                return (int(avg_x), int(avg_y), int(avg_w), int(avg_h))

        return candidate_boxes[0][0]

    def _adaptive_box_refinement(self, box: Tuple[int, int, int, int],
                                 img_array: np.ndarray,
                                 object_type: ObjectType) -> Tuple[int, int, int, int]:
        """è‡ªé€‚åº”è¾¹ç•Œç»†åŒ–"""
        x, y, w, h = box
        img_h, img_w = img_array.shape[:2]

        # æ ¹æ®å¯¹è±¡ç±»å‹è°ƒæ•´ç»†åŒ–ç­–ç•¥
        if object_type == ObjectType.RELIEF:
            # æµ®é›•éœ€è¦æ›´ç´§çš„è¾¹ç•Œ
            shrink_ratio = 0.03
        elif object_type == ObjectType.CARVING:
            # é›•åˆ»å“å¯èƒ½éœ€è¦ç¨å¾®æ‰©å±•ä»¥åŒ…å«ç»†èŠ‚
            shrink_ratio = -0.015
        elif object_type == ObjectType.FLOWER:
            # èŠ±è‰ä¿æŒåŸæœ‰è¾¹ç•Œ
            shrink_ratio = 0.01
        else:
            shrink_ratio = 0.02

        # åº”ç”¨è°ƒæ•´
        shrink_x = int(w * abs(shrink_ratio))
        shrink_y = int(h * abs(shrink_ratio))

        if shrink_ratio >= 0:
            # æ”¶ç¼©
            x = max(0, x + shrink_x)
            y = max(0, y + shrink_y)
            w = max(50, min(img_w - x, w - 2 * shrink_x))
            h = max(50, min(img_h - y, h - 2 * shrink_y))
        else:
            # æ‰©å±•
            x = max(0, x - shrink_x)
            y = max(0, y - shrink_y)
            w = min(img_w - x, w + 2 * shrink_x)
            h = min(img_h - y, h + 2 * shrink_y)

        # æœ€ç»ˆè¾¹ç•Œæ£€æŸ¥
        if x + w > img_w:
            w = img_w - x
        if y + h > img_h:
            h = img_h - y

        return (x, y, w, h)

    def _intelligent_fallback_box(self, img_array: np.ndarray,
                                  object_type: ObjectType) -> Tuple[int, int, int, int]:
        """æ™ºèƒ½é™çº§è¾¹ç•Œæ¡†"""
        h, w = img_array.shape[:2]

        # æ ¹æ®å¯¹è±¡ç±»å‹è°ƒæ•´é™çº§ç­–ç•¥
        if object_type == ObjectType.CARVING:
            margin_ratio = 0.05  # é›•åˆ»å“é€šå¸¸éœ€è¦æ›´å°‘çš„è¾¹è·
        elif object_type == ObjectType.RELIEF:
            margin_ratio = 0.08  # æµ®é›•éœ€è¦ä¸­ç­‰è¾¹è·
        else:
            margin_ratio = 0.12  # å…¶ä»–ç±»å‹æ›´ä¿å®ˆ

        print(f"  âš ï¸ æ‰€æœ‰æ£€æµ‹æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨æ™ºèƒ½é™çº§è¾¹ç•Œ")

        x = int(w * margin_ratio)
        y = int(h * margin_ratio)
        box_w = int(w * (1 - 2 * margin_ratio))
        box_h = int(h * (1 - 2 * margin_ratio))

        return (x, y, box_w, box_h)

    # ============= åˆå§‹æ©ç ç”Ÿæˆï¼ˆä¿æŒä¼˜åŒ–é€»è¾‘ï¼‰ =============

    def create_initial_mask(self, image: Image.Image,
                            content_box: Tuple[int, int, int, int],
                            object_type: ObjectType) -> np.ndarray:
        """åˆ›å»ºè‡ªé€‚åº”åˆå§‹æ©ç """
        img_array = np.array(image)
        x, y, w, h = content_box

        # è¾¹ç•Œæ£€æŸ¥
        img_h, img_w = img_array.shape[:2]
        x = max(0, min(x, img_w - 1))
        y = max(0, min(y, img_h - 1))
        w = min(w, img_w - x)
        h = min(h, img_h - y)

        if w <= 0 or h <= 0:
            print(f"  âš ï¸ æ— æ•ˆçš„å†…å®¹æ¡†: ({x}, {y}, {w}, {h})")
            return np.zeros((img_h, img_w), dtype=bool)

        roi_img = img_array[y:y + h, x:x + w]

        if roi_img.size == 0:
            return np.zeros((h, w), dtype=bool)

        # æ ¹æ®å¯¹è±¡ç±»å‹ä¼˜åŒ–å‚æ•°
        params = {
            'points_per_side': self.config.points_per_side,
            'pred_iou_thresh': 0.85,
            'stability_score_thresh': 0.92,
            'min_mask_region_area': self.config.min_mask_region_area
        }

        if object_type == ObjectType.CARVING:
            params.update({
                'points_per_side': 64,
                'pred_iou_thresh': 0.88,
                'stability_score_thresh': 0.95,
                'min_mask_region_area': 25
            })
        elif object_type == ObjectType.RELIEF:
            params.update({
                'points_per_side': 84,
                'pred_iou_thresh': 0.75,
                'stability_score_thresh': 0.85,
                'min_mask_region_area': 10
            })
        elif object_type == ObjectType.FLOWER:
            params.update({
                'points_per_side': 72,
                'pred_iou_thresh': 0.80,
                'stability_score_thresh': 0.88,
                'min_mask_region_area': 15
            })

        mask_generator = SamAutomaticMaskGenerator(model=self.sam, **params)
        print(f"  ğŸ¤– è¿è¡Œè‡ªåŠ¨åˆ†å‰²å™¨ (ç±»å‹: {object_type.value})...")
        masks = mask_generator.generate(roi_img)

        if not masks:
            return np.zeros((h, w), dtype=bool)

        # æ”¹è¿›çš„æ©ç ç­›é€‰
        good_masks = self._improved_mask_filtering(masks, roi_img, object_type)
        if not good_masks:
            good_masks = self._fallback_mask_selection(masks, roi_img)

        print(f"  âœ… ç­›é€‰å‡º {len(good_masks)} ä¸ªåˆæ ¼ç¢ç‰‡")

        initial_mask = np.zeros(roi_img.shape[:2], dtype=bool)
        for mask in good_masks:
            initial_mask = np.logical_or(initial_mask, mask)

        # å½¢æ€å­¦å¤„ç†
        initial_mask = self._adaptive_morphological_processing(initial_mask, object_type)
        return initial_mask

    def _improved_mask_filtering(self, masks: List[Dict], roi_img: np.ndarray,
                                 object_type: ObjectType) -> List[np.ndarray]:
        """æ”¹è¿›çš„æ©ç ç­›é€‰"""
        if object_type == ObjectType.CARVING:
            return self._filter_carving_fragments_v2(masks, roi_img)
        elif object_type == ObjectType.RELIEF:
            return self._filter_relief_fragments_v2(masks, roi_img)
        elif object_type == ObjectType.FLOWER:
            return self._filter_flower_fragments_v2(masks, roi_img)
        else:
            return self._filter_standard_fragments_v2(masks, roi_img)

    def _filter_carving_fragments_v2(self, masks: List[Dict],
                                     roi_img: np.ndarray) -> List[np.ndarray]:
        """æ”¹è¿›çš„é›•åˆ»å“ç¢ç‰‡ç­›é€‰"""
        gray_roi = cv2.cvtColor(roi_img, cv2.COLOR_RGB2GRAY)
        roi_h, roi_w = roi_img.shape[:2]

        # å¢å¼ºç‰¹å¾è®¡ç®—
        edges = cv2.Canny(gray_roi, 20, 100)
        texture_map = np.abs(cv2.Laplacian(gray_roi, cv2.CV_64F))

        # èƒŒæ™¯è‰²ä¼°è®¡
        border_size = min(10, min(roi_h, roi_w) // 8)
        if border_size > 0:
            border_pixels = np.concatenate([
                roi_img[:border_size, :].reshape(-1, 3),
                roi_img[-border_size:, :].reshape(-1, 3),
                roi_img[:, :border_size].reshape(-1, 3),
                roi_img[:, -border_size:].reshape(-1, 3)
            ])
            bg_color = np.median(border_pixels, axis=0)
        else:
            bg_color = np.array([128, 128, 128])

        candidates = []
        center_x, center_y = roi_w / 2, roi_h / 2

        for m in masks:
            mask, bbox, area = m['segmentation'], m['bbox'], m['area']
            area_ratio = area / (roi_w * roi_h)

            if area_ratio < 0.001 or area_ratio > 0.8:
                continue

            # é¢œè‰²å·®å¼‚
            mask_pixels = roi_img[mask]
            if len(mask_pixels) > 0:
                mask_color = np.median(mask_pixels, axis=0)
                color_diff = np.linalg.norm(mask_color - bg_color)
            else:
                color_diff = 0

            if color_diff < 10:  # ä¸èƒŒæ™¯é¢œè‰²å¤ªç›¸ä¼¼
                continue

            # ä½ç½®è¯„åˆ†
            cx, cy = bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2
            center_score = 1 - (np.sqrt((cx - center_x) ** 2 + (cy - center_y) ** 2) /
                                np.sqrt(center_x ** 2 + center_y ** 2))

            # è¾¹ç¼˜ç‰¹å¾è¯„åˆ†
            edge_score = np.mean(edges[mask] > 0) if mask.any() else 0

            # çº¹ç†ç‰¹å¾è¯„åˆ†
            texture_score = np.mean(texture_map[mask]) / (np.mean(texture_map) + 1e-6)
            texture_score = min(texture_score, 2.0)  # é™åˆ¶ä¸Šé™

            # å½¢çŠ¶è§„æ•´æ€§
            if bbox[2] > 0 and bbox[3] > 0:
                aspect_ratio = bbox[2] / bbox[3]
                shape_score = 1 - abs(np.log(aspect_ratio)) * 0.5
                shape_score = max(0, min(1, shape_score))
            else:
                shape_score = 0

            # ç»¼åˆè¯„åˆ†
            total_score = (center_score * 0.2 + edge_score * 0.3 +
                           texture_score * 0.25 + shape_score * 0.15 +
                           (color_diff / 100.0) * 0.1)

            if total_score > 0.25 or area_ratio > 0.05:
                candidates.append({
                    'mask': mask,
                    'score': total_score,
                    'area_ratio': area_ratio
                })

        # é€‰æ‹©æœ€ä½³å€™é€‰
        candidates.sort(key=lambda x: x['score'], reverse=True)

        good_masks = []
        total_area = 0
        max_coverage = 0.75

        for cand in candidates:
            good_masks.append(cand['mask'])
            total_area += np.sum(cand['mask'])
            if total_area / (roi_w * roi_h) > max_coverage:
                break
            if len(good_masks) >= 15:  # é™åˆ¶æ•°é‡
                break

        return good_masks

    def _filter_relief_fragments_v2(self, masks: List[Dict],
                                    roi_img: np.ndarray) -> List[np.ndarray]:
        """æ”¹è¿›çš„æµ®é›•ç¢ç‰‡ç­›é€‰"""
        gray_roi = cv2.cvtColor(roi_img, cv2.COLOR_RGB2GRAY)
        roi_h, roi_w = roi_img.shape[:2]

        # å¯¹æ¯”åº¦å¢å¼º
        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))
        enhanced_gray = clahe.apply(gray_roi)

        # ç‰¹å¾æå–
        edges = cv2.Canny(enhanced_gray, 15, 60)
        texture_map = np.abs(cv2.Laplacian(enhanced_gray, cv2.CV_64F))

        candidates = []
        center_x, center_y = roi_w / 2, roi_h / 2

        for m in masks:
            mask, bbox, area = m['segmentation'], m['bbox'], m['area']
            area_ratio = area / (roi_w * roi_h)

            if area_ratio < 0.001:
                continue

            # ä½ç½®è¯„åˆ†
            cx, cy = bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2
            center_score = 1 - (np.sqrt((cx - center_x) ** 2 + (cy - center_y) ** 2) /
                                np.sqrt(center_x ** 2 + center_y ** 2))

            # è¾¹ç¼˜å¯†åº¦
            edge_score = np.mean(edges[mask] > 0) if mask.any() else 0

            # çº¹ç†ä¸€è‡´æ€§ï¼ˆæµ®é›•é€šå¸¸çº¹ç†è¾ƒä¸ºå‡åŒ€ï¼‰
            if mask.any():
                mask_texture = texture_map[mask]
                texture_std = np.std(mask_texture)
                texture_consistency = 1 / (1 + texture_std / 50.0)  # æ ‡å‡†å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜
            else:
                texture_consistency = 0

            # äº®åº¦å‡åŒ€æ€§
            if mask.any():
                mask_brightness = enhanced_gray[mask]
                brightness_std = np.std(mask_brightness)
                brightness_consistency = 1 / (1 + brightness_std / 30.0)
            else:
                brightness_consistency = 0

            # ç»¼åˆè¯„åˆ†
            total_score = (center_score * 0.25 + edge_score * 0.25 +
                           texture_consistency * 0.25 + brightness_consistency * 0.25)

            if total_score > 0.2 or area_ratio > 0.03:
                candidates.append({
                    'mask': mask,
                    'score': total_score,
                    'area_ratio': area_ratio
                })

        candidates.sort(key=lambda x: x['score'], reverse=True)

        good_masks = []
        total_area = 0
        for cand in candidates:
            good_masks.append(cand['mask'])
            total_area += np.sum(cand['mask'])
            if total_area / (roi_w * roi_h) > 0.8:
                break
            if len(good_masks) >= 20:
                break

        return good_masks

    def _filter_flower_fragments_v2(self, masks: List[Dict],
                                    roi_img: np.ndarray) -> List[np.ndarray]:
        """æ”¹è¿›çš„èŠ±è‰ç¢ç‰‡ç­›é€‰"""
        roi_h, roi_w = roi_img.shape[:2]

        # æŒ‰é¢ç§¯å’Œä½ç½®ç»¼åˆæ’åº
        enhanced_masks = []
        center_x, center_y = roi_w / 2, roi_h / 2

        for m in masks:
            mask, bbox, area = m['segmentation'], m['bbox'], m['area']
            area_ratio = area / (roi_w * roi_h)

            if area_ratio < 0.002:
                continue

            # ä½ç½®è¯„åˆ†
            cx, cy = bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2
            center_distance = np.sqrt((cx - center_x) ** 2 + (cy - center_y) ** 2)
            max_distance = np.sqrt(center_x ** 2 + center_y ** 2)
            position_score = 1 - center_distance / max_distance

            # å½¢çŠ¶å¤æ‚åº¦ï¼ˆèŠ±è‰é€šå¸¸å½¢çŠ¶å¤æ‚ï¼‰
            contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if contours:
                perimeter = cv2.arcLength(contours[0], True)
                if area > 0:
                    complexity = perimeter * perimeter / area  # å‘¨é•¿å¹³æ–¹ä¸é¢ç§¯æ¯”
                    complexity_score = min(complexity / 50.0, 1.0)  # å½’ä¸€åŒ–
                else:
                    complexity_score = 0
            else:
                complexity_score = 0

            # ç»¼åˆè¯„åˆ†
            total_score = area_ratio * 0.4 + position_score * 0.3 + complexity_score * 0.3

            enhanced_masks.append({
                'mask': mask,
                'score': total_score,
                'area': area
            })

        # æ’åºå¹¶é€‰æ‹©
        enhanced_masks.sort(key=lambda x: x['score'], reverse=True)

        good_masks = []
        total_area = 0
        for item in enhanced_masks:
            good_masks.append(item['mask'])
            total_area += item['area']
            if total_area / (roi_w * roi_h) > 0.85:
                break
            if len(good_masks) >= 25:
                break

        return good_masks

    def _filter_standard_fragments_v2(self, masks: List[Dict],
                                      roi_img: np.ndarray) -> List[np.ndarray]:
        """æ”¹è¿›çš„æ ‡å‡†ç¢ç‰‡ç­›é€‰"""
        roi_h, roi_w = roi_img.shape[:2]
        good_masks = []

        # æŒ‰é¢ç§¯æ’åº
        masks_sorted = sorted(masks, key=lambda x: x['area'], reverse=True)

        for m in masks_sorted:
            bbox, area = m['bbox'], m['area']
            area_ratio = area / (roi_w * roi_h)

            if area_ratio < 0.005:
                continue

            # è¾¹ç¼˜æ£€æŸ¥ï¼ˆé¿å…è¾¹ç¼˜å°ç¢ç‰‡ï¼‰
            margin = 8
            is_on_edge = (bbox[0] < margin or bbox[1] < margin or
                          (bbox[0] + bbox[2]) > roi_w - margin or
                          (bbox[1] + bbox[3]) > roi_h - margin)

            if is_on_edge and area_ratio < 0.15:
                continue

            # å½¢çŠ¶åˆç†æ€§æ£€æŸ¥
            if bbox[2] > 0 and bbox[3] > 0:
                aspect_ratio = bbox[2] / bbox[3]
                if 0.1 < aspect_ratio < 10.0:  # åˆç†çš„å®½é«˜æ¯”
                    good_masks.append(m['segmentation'])

            if len(good_masks) >= 10:
                break

        return good_masks

    def _fallback_mask_selection(self, masks: List[Dict],
                                 roi_img: np.ndarray) -> List[np.ndarray]:
        """é™çº§æ©ç é€‰æ‹©"""
        if not masks:
            return []

        roi_h, roi_w = roi_img.shape[:2]
        masks_sorted = sorted(masks, key=lambda x: x['area'], reverse=True)

        selected = []
        total_area = 0
        target_coverage = 0.6

        for m in masks_sorted:
            selected.append(m['segmentation'])
            total_area += m['area']

            if total_area / (roi_w * roi_h) > target_coverage or len(selected) >= 8:
                break

        return selected

    def _adaptive_morphological_processing(self, mask: np.ndarray,
                                           object_type: ObjectType) -> np.ndarray:
        """è‡ªé€‚åº”å½¢æ€å­¦å¤„ç†"""
        mask_uint8 = mask.astype(np.uint8) * 255

        if object_type == ObjectType.CARVING:
            # é›•åˆ»å“ï¼šç²¾ç»†å¤„ç†ï¼Œä¿æŒç»†èŠ‚
            kernel_small = np.ones((2, 2), np.uint8)
            mask_uint8 = cv2.morphologyEx(mask_uint8, cv2.MORPH_CLOSE, kernel_small, iterations=1)

            # å¡«å……å°å­”æ´
            filled = ndimage.binary_fill_holes(mask_uint8 > 0).astype(np.uint8) * 255

            # åªå¡«å……å°çš„å­”æ´
            diff = filled - mask_uint8
            kernel_filter = np.ones((3, 3), np.uint8)
            diff_filtered = cv2.morphologyEx(diff, cv2.MORPH_OPEN, kernel_filter, iterations=1)

            mask_uint8 = mask_uint8 + diff_filtered

            # è½»å¾®å¹³æ»‘
            kernel_smooth = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            mask_uint8 = cv2.morphologyEx(mask_uint8, cv2.MORPH_OPEN, kernel_smooth, iterations=1)

        elif object_type == ObjectType.RELIEF:
            # æµ®é›•ï¼šæ›´å¼ºçš„è¿æ¥å’Œå¡«å……
            kernel_medium = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
            mask_uint8 = cv2.morphologyEx(mask_uint8, cv2.MORPH_CLOSE, kernel_medium, iterations=2)

            # å¡«å……å­”æ´
            mask_uint8 = ndimage.binary_fill_holes(mask_uint8 > 0).astype(np.uint8) * 255

            # å¹³æ»‘å¤„ç†
            mask_uint8 = cv2.medianBlur(mask_uint8, 3)

        elif object_type == ObjectType.FLOWER:
            # èŠ±è‰ï¼šä¿æŒå½¢çŠ¶å¤æ‚æ€§
            kernel_adaptive = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            mask_uint8 = cv2.morphologyEx(mask_uint8, cv2.MORPH_CLOSE, kernel_adaptive, iterations=1)

            # è½»å¾®å¡«å……
            filled = ndimage.binary_fill_holes(mask_uint8 > 0).astype(np.uint8) * 255

            # ä¿å®ˆèåˆ
            diff = filled - mask_uint8
            small_holes = cv2.morphologyEx(diff, cv2.MORPH_OPEN, np.ones((2, 2), np.uint8))
            mask_uint8 = mask_uint8 + small_holes

        else:
            # æ ‡å‡†å¤„ç†
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            mask_uint8 = cv2.morphologyEx(mask_uint8, cv2.MORPH_CLOSE, kernel, iterations=1)
            mask_uint8 = cv2.morphologyEx(mask_uint8, cv2.MORPH_OPEN, kernel, iterations=1)

        return mask_uint8 > 0

    # ============= è¾…åŠ©åŠŸèƒ½ =============

    def set_image_for_predictor(self, image: np.ndarray):
        """ä¸ºé¢„æµ‹å™¨è®¾ç½®å›¾åƒ"""
        self.predictor.set_image(image)

    def get_predictor(self) -> SamPredictor:
        """è·å–SAMé¢„æµ‹å™¨"""
        return self.predictor

    def get_mask_generator(self, **kwargs) -> SamAutomaticMaskGenerator:
        """è·å–æ©ç ç”Ÿæˆå™¨"""
        return SamAutomaticMaskGenerator(model=self.sam, **kwargs)