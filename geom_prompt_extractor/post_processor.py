# geom_prompt_extractor/post_processor.py
"""
Part 3: åå¤„ç†ä¸æ‰¹é‡å¤„ç† - æ€§èƒ½ä¼˜åŒ–ç‰ˆ
åŒ…å«ï¼šå†…å­˜ç®¡ç†ã€è¶…æ—¶æœºåˆ¶ã€ç¼“å­˜ä¼˜åŒ–ã€æ‰¹é‡å¤„ç†ä¼˜åŒ–
"""

import torch
import numpy as np
import cv2
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from pathlib import Path
import shutil
from typing import Tuple, List, Dict, Optional
import scipy.ndimage as ndimage
import time
import gc
import signal
from contextlib import contextmanager
import hashlib

# ä»é…ç½®æ¨¡å—å¯¼å…¥
from .config import ExtractionConfig, ObjectType
# å¯¼å…¥å…¶ä»–æ¨¡å—
from .sam_core import SAMCore
from .segmentation_processor import SegmentationProcessor

# è®¾ç½®matplotlibä¸­æ–‡æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


# è¶…æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨
@contextmanager
def timeout(seconds):
    """è¶…æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œé˜²æ­¢å¤„ç†å¡æ­»"""

    def timeout_handler(signum, frame):
        raise TimeoutError(f"å¤„ç†è¶…è¿‡{seconds}ç§’è¶…æ—¶")

    # Windowsç³»ç»Ÿä¸æ”¯æŒSIGALRMï¼Œä½¿ç”¨çº¿ç¨‹æ›¿ä»£
    import platform
    if platform.system() == 'Windows':
        import threading
        timer = threading.Timer(seconds, lambda: None)
        timer.start()
        try:
            yield
        finally:
            timer.cancel()
    else:
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(seconds)
        try:
            yield
        finally:
            signal.alarm(0)


class PostProcessor:
    """åå¤„ç†å™¨ - å¤„ç†æ©ç ä¼˜åŒ–ã€å‰æ™¯æå–å’Œå¯è§†åŒ–"""

    def __init__(self, sam_core: SAMCore, config: ExtractionConfig = None):
        """åˆå§‹åŒ–åå¤„ç†å™¨"""
        self.sam_core = sam_core
        self.config = config or ExtractionConfig()
        # æ·»åŠ ç¼“å­˜å­—å…¸
        self._processing_cache = {}

    def clear_cache(self):
        """æ¸…ç†å¤„ç†ç¼“å­˜"""
        self._processing_cache.clear()
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def post_process_mask_enhanced(self, mask: torch.Tensor,
                                   content_box: Tuple[int, int, int, int],
                                   object_type: ObjectType,
                                   method: str,
                                   image: np.ndarray = None,
                                   fast_mode: bool = False) -> torch.Tensor:
        """V8ä¼˜åŒ–ç‰ˆåå¤„ç† - æ·»åŠ å¿«é€Ÿæ¨¡å¼é€‰é¡¹"""
        mask_np = mask.cpu().numpy().astype(np.uint8)
        h, w = mask_np.shape
        x, y, bw, bh = content_box

        # 1. ä¸¥æ ¼çš„è¾¹ç•Œè£å‰ª
        cleaned_mask = np.zeros_like(mask_np)
        margin = 1 if method == 'geometric_points' else 2
        x_start, y_start = max(0, x + margin), max(0, y + margin)
        x_end, y_end = min(w, x + bw - margin), min(h, y + bh - margin)

        if x_start < x_end and y_start < y_end:
            cleaned_mask[y_start:y_end, x_start:x_end] = mask_np[y_start:y_end, x_start:x_end]

        # å¿«é€Ÿæ¨¡å¼è·³è¿‡å¤æ‚å¤„ç†
        if fast_mode:
            return torch.from_numpy(cleaned_mask > 0)

        # 2. å¯¹æµ®é›•ç±»å‹ç‰¹æ®Šå¤„ç†
        if object_type == ObjectType.RELIEF:
            cleaned_mask = self._process_relief_mask(cleaned_mask, content_box)
        else:
            # å…¶ä»–ç±»å‹çš„è¾¹ç¼˜æ”¶ç¼©
            if image is not None and len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                edges = cv2.Canny(gray, 50, 150)

                kernel = np.ones((3, 3), np.uint8)
                dilated = cv2.dilate(cleaned_mask, kernel, iterations=1)
                eroded = cv2.erode(cleaned_mask, kernel, iterations=1)
                mask_edge = dilated - eroded

                strong_edges = (edges > 100) & (mask_edge > 0)
                cleaned_mask[strong_edges] = 0

        # 3. è¿é€šåŸŸåˆ†æ
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(cleaned_mask)

        if num_labels > 1:
            areas = stats[1:, cv2.CC_STAT_AREA]
            if len(areas) > 0:
                max_area_idx = np.argmax(areas) + 1

                cleaned_mask = np.zeros_like(cleaned_mask)
                cleaned_mask[labels == max_area_idx] = 1

                # å¯¹äºæµ®é›•ï¼Œä¿ç•™æ›´å¤šçš„ç»„ä»¶
                threshold = 0.03 if object_type == ObjectType.RELIEF else 0.05

                for i in range(1, num_labels):
                    if i == max_area_idx:
                        continue
                    area = stats[i, cv2.CC_STAT_AREA]
                    if area > bw * bh * threshold:
                        cx, cy = centroids[i]
                        center_dist = np.sqrt((cx - w / 2) ** 2 + (cy - h / 2) ** 2)
                        max_dist = 0.7 if object_type == ObjectType.RELIEF else 0.5
                        if center_dist < np.sqrt((w / 2) ** 2 + (h / 2) ** 2) * max_dist:
                            cleaned_mask[labels == i] = 1

        # 4. å½¢æ€å­¦æ“ä½œï¼ˆå¯é€‰ï¼‰
        if self.config.adaptive_morphology and not fast_mode:
            if object_type == ObjectType.CARVING:
                kernel_tiny = np.ones((2, 2), np.uint8)
                cleaned_mask = cv2.morphologyEx(cleaned_mask, cv2.MORPH_CLOSE, kernel_tiny, iterations=1)

                filled = ndimage.binary_fill_holes(cleaned_mask).astype(np.uint8)

                diff = filled - cleaned_mask
                if np.sum(diff) < bw * bh * 0.1:
                    cleaned_mask = filled

                kernel_smooth = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                cleaned_mask = cv2.morphologyEx(cleaned_mask, cv2.MORPH_OPEN, kernel_smooth, iterations=1)

            elif object_type == ObjectType.RELIEF:
                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
                cleaned_mask = cv2.morphologyEx(cleaned_mask, cv2.MORPH_CLOSE, kernel, iterations=2)
                cleaned_mask = ndimage.binary_fill_holes(cleaned_mask).astype(np.uint8)
                kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                cleaned_mask = cv2.morphologyEx(cleaned_mask, cv2.MORPH_OPEN, kernel_small, iterations=1)

            else:
                kernel = np.ones((2, 2), np.uint8)
                cleaned_mask = cv2.morphologyEx(cleaned_mask, cv2.MORPH_CLOSE, kernel, iterations=1)

        # 5. æœ€ç»ˆçš„è¾¹ç¼˜å¹³æ»‘ï¼ˆå¯é€‰ï¼‰
        if self.config.smooth_edges and not fast_mode:
            cleaned_mask_float = cleaned_mask.astype(np.float32)
            if object_type == ObjectType.RELIEF:
                smoothed = cv2.bilateralFilter(cleaned_mask_float, 5, 75, 75)
                cleaned_mask = (smoothed > 0.3).astype(np.uint8)
            else:
                smoothed = cv2.bilateralFilter(cleaned_mask_float, 3, 50, 50)
                cleaned_mask = (smoothed > 0.5).astype(np.uint8)

        return torch.from_numpy(cleaned_mask > 0)

    def _process_relief_mask(self, mask: np.ndarray,
                             content_box: Tuple[int, int, int, int]) -> np.ndarray:
        """ä¸“é—¨å¤„ç†æµ®é›•æ©ç """
        x, y, bw, bh = content_box

        # ä½¿ç”¨è·ç¦»å˜æ¢æ‰¾åˆ°ä¸»ä½“åŒºåŸŸ
        dist_transform = cv2.distanceTransform(mask, cv2.DIST_L2, 5)

        # æ‰¾åˆ°è·ç¦»å˜æ¢çš„å³°å€¼ç‚¹ä½œä¸ºç§å­ç‚¹
        _, max_val, _, max_loc = cv2.minMaxLoc(dist_transform)

        # ä½¿ç”¨åˆ†æ°´å²­ç®—æ³•æ”¹å–„åˆ†å‰²
        if max_val > 0:
            markers = np.zeros_like(mask)
            markers[max_loc[1], max_loc[0]] = 1
            markers[0, :] = 2
            markers[-1, :] = 2
            markers[:, 0] = 2
            markers[:, -1] = 2

            mask_3channel = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
            markers = cv2.watershed(mask_3channel, markers.astype(np.int32))
            refined_mask = (markers == 1).astype(np.uint8)
            combined_mask = np.logical_or(mask > 0, refined_mask > 0).astype(np.uint8)

            return combined_mask

        return mask

    def extract_foreground(self, image: Image.Image, mask: torch.Tensor,
                           content_box: Tuple[int, int, int, int] = None,
                           crop_to_content: bool = True) -> Tuple[np.ndarray, int]:
        """æå–å‰æ™¯"""
        img_array = np.array(image)
        mask_array = mask.cpu().numpy()

        # æ£€æµ‹å¯¹è±¡ç±»å‹ç”¨äºè°ƒæ•´ç¾½åŒ–
        object_type = self.sam_core.detect_object_type(Image.fromarray(img_array))

        if crop_to_content and content_box is not None:
            x, y, bw, bh = content_box
            cropped_img = img_array[y:y + bh, x:x + bw]
            cropped_mask = mask_array[y:y + bh, x:x + bw]

            alpha = (cropped_mask * 255).astype(np.uint8)
            if self.config.smooth_edges:
                feather_radius = 3 if object_type == ObjectType.RELIEF else self.config.feather_radius
                alpha = cv2.GaussianBlur(alpha,
                                         (feather_radius * 2 + 1,
                                          feather_radius * 2 + 1), 0)

            foreground_rgba = cv2.cvtColor(cropped_img, cv2.COLOR_RGB2RGBA)
            foreground_rgba[:, :, 3] = alpha
            return foreground_rgba, np.sum(cropped_mask)
        else:
            alpha = (mask_array * 255).astype(np.uint8)
            foreground_rgba = cv2.cvtColor(img_array, cv2.COLOR_RGB2RGBA)
            foreground_rgba[:, :, 3] = alpha
            return foreground_rgba, np.sum(mask_array)

    def visualize_results(self, image: Image.Image,
                          content_box: Tuple[int, int, int, int],
                          initial_mask: np.ndarray,
                          geometric_points: np.ndarray,
                          final_mask: torch.Tensor,
                          score: float,
                          object_type: ObjectType,
                          method: str,
                          filename: str,
                          output_path: Path,
                          save_visualization: bool = True):
        """åˆ›å»ºå¯è§†åŒ–ç»“æœ - æ·»åŠ ä¿å­˜æ§åˆ¶é€‰é¡¹"""
        if not save_visualization:
            return

        fig, axes = plt.subplots(2, 3, figsize=(20, 13))
        img_array = np.array(image)
        x, y, w, h = content_box

        # 1. Content Box
        axes[0, 0].imshow(image)
        rect = patches.Rectangle((x, y), w, h, lw=3, ec='yellow', fc='none', ls='--')
        axes[0, 0].add_patch(rect)
        axes[0, 0].set_title(f'å†…å®¹æ¡†æ£€æµ‹ (ç±»å‹: {object_type.value})')

        # 2. Initial Mask
        axes[0, 1].imshow(image)
        if initial_mask is not None and initial_mask.any():
            full_initial_mask = np.zeros_like(img_array, dtype=np.uint8)
            initial_mask_resized = cv2.resize(initial_mask.astype(np.uint8), (w, h))
            full_initial_mask[y:y + h, x:x + w, 0] = initial_mask_resized * 255
            axes[0, 1].imshow(full_initial_mask, alpha=0.5)
        axes[0, 1].set_title('æ™ºèƒ½ç¢ç‰‡èšåˆ')

        # 3. Geometric Points
        axes[0, 2].imshow(image)
        if geometric_points is not None and len(geometric_points) > 0:
            points_for_display = np.array([[p[1] + x, p[0] + y] for p in geometric_points])
            axes[0, 2].scatter(points_for_display[:, 0], points_for_display[:, 1],
                               color='lime', s=25, marker='*')
        axes[0, 2].set_title('V8å‡ ä½•æ„ŸçŸ¥ç‚¹åˆ†å¸ƒ')

        # 4. Final Mask Overlay
        axes[1, 0].imshow(image)
        final_mask_overlay = np.zeros_like(img_array)
        final_mask_overlay[final_mask.cpu().numpy()] = [138, 43, 226]
        axes[1, 0].imshow(final_mask_overlay, alpha=0.6)
        axes[1, 0].set_title(f'å¤šç­–ç•¥SAMåˆ†å‰² ({method}, IoU: {score:.3f})')

        # 5. Final Mask
        axes[1, 1].imshow(final_mask.cpu().numpy(), cmap='gray')
        title = 'V8è‡ªé€‚åº”åå¤„ç†æ©ç '
        if object_type == ObjectType.RELIEF:
            title += ' (æµ®é›•ä¼˜åŒ–)'
        axes[1, 1].set_title(title)

        # 6. Extracted Foreground
        foreground_only, _ = self.extract_foreground(image, final_mask, content_box, crop_to_content=True)
        axes[1, 2].imshow(foreground_only)
        axes[1, 2].set_title('å‰æ™¯æå–ç»“æœ')

        for ax in axes.ravel():
            ax.axis('off')

        plt.suptitle(f'V8 Final: {filename}')
        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.savefig(output_path / f"{filename}_v8_analysis.png", dpi=120)
        plt.close('all')  # ç¡®ä¿å…³é—­æ‰€æœ‰å›¾å½¢

        # æ¸…ç†matplotlibç¼“å­˜
        plt.clf()
        plt.cla()
        gc.collect()


class GeomPromptExtractorV8:
    """GeomPrompt V8ä¸»ç±» - æ€§èƒ½ä¼˜åŒ–ç‰ˆ"""

    def __init__(self, checkpoint_path: str, model_type: str = 'vit_h',
                 device: str = None, force_gpu: bool = True):
        """åˆå§‹åŒ–V8æå–å™¨"""
        # åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å—
        self.sam_core = SAMCore(checkpoint_path, model_type, device, force_gpu)
        self.config = self.sam_core.config

        # åˆå§‹åŒ–å¤„ç†å™¨
        self.seg_processor = SegmentationProcessor(self.sam_core, self.config)
        self.post_processor = PostProcessor(self.sam_core, self.config)

        # å…¼å®¹æ€§å±æ€§
        self.sam = self.sam_core.sam
        self.predictor = self.sam_core.predictor
        self.device = self.sam_core.device

        # æ€§èƒ½ç»Ÿè®¡
        self.processing_times = []

        # å¿«é€Ÿæ¨¡å¼æ ‡å¿—
        self.fast_mode = False

    def enable_fast_mode(self):
        """å¯ç”¨å¿«é€Ÿæ¨¡å¼"""
        self.fast_mode = True
        self.config.points_per_side = 32
        self.config.enable_multi_prompt = False
        self.config.smooth_edges = False
        self.config.adaptive_morphology = False
        print("âš¡ å¿«é€Ÿæ¨¡å¼å·²å¯ç”¨")

    def disable_fast_mode(self):
        """ç¦ç”¨å¿«é€Ÿæ¨¡å¼"""
        self.fast_mode = False
        self.config = ExtractionConfig()  # æ¢å¤é»˜è®¤é…ç½®
        print("ğŸ”§ å¿«é€Ÿæ¨¡å¼å·²ç¦ç”¨")

    def adjust_config_for_object_type(self, object_type: ObjectType):
        """æ ¹æ®å¯¹è±¡ç±»å‹è‡ªåŠ¨è°ƒæ•´é…ç½®"""
        if self.fast_mode:
            return  # å¿«é€Ÿæ¨¡å¼ä¸‹ä¸è°ƒæ•´

        if object_type == ObjectType.RELIEF:
            print("  ğŸ“‹ åº”ç”¨æµ®é›•ä¼˜åŒ–é…ç½®...")
            self.config.points_per_side = 84
            self.config.pred_iou_thresh = 0.82
            self.config.stability_score_thresh = 0.88
            self.config.min_mask_region_area = 8
            self.config.geometric_point_ratio = 0.9
            self.config.corner_detection_quality = 0.0003
            self.config.curvature_threshold = 0.02
            self.config.min_quality_threshold = 0.65
            self.config.feather_radius = 3

    def adjust_points_by_size(self, image_size: Tuple[int, int]) -> int:
        """æ ¹æ®å›¾åƒå¤§å°åŠ¨æ€è°ƒæ•´é‡‡æ ·ç‚¹æ•°é‡"""
        w, h = image_size
        area = w * h

        if self.fast_mode:
            return 40  # å¿«é€Ÿæ¨¡å¼å›ºå®š40ä¸ªç‚¹

        if area < 500 * 500:
            return 80
        elif area < 1000 * 1000:
            return 120
        else:
            return 150

    def process_with_extraction(self, image_path: str,
                                output_dir: str = "extraction_results_v8",
                                force_overwrite: bool = True,
                                timeout_seconds: int = 60,
                                save_visualization: bool = True) -> Optional[Dict]:
        """å¤„ç†å•å¼ å›¾åƒ - æ·»åŠ è¶…æ—¶å’Œå†…å­˜ç®¡ç†"""
        start_time = time.time()
        filename = Path(image_path).stem

        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        foreground_file = output_path / f"{filename}_foreground.png"
        if not force_overwrite and foreground_file.exists():
            print(f"âš ï¸ æ–‡ä»¶ {filename} å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            return None

        print(f"\nğŸ“· GeomPromptå‰æ™¯æå– V8: {filename}")

        try:
            # ä½¿ç”¨è¶…æ—¶æœºåˆ¶
            with timeout(timeout_seconds):
                # åŠ è½½å›¾åƒ
                image = Image.open(image_path).convert("RGB")
                print(f"   å°ºå¯¸: {image.width}x{image.height}")

                # æ£€æµ‹å¯¹è±¡ç±»å‹
                object_type = self.sam_core.detect_object_type(image)
                print(f"ğŸ” æ£€æµ‹åˆ°å¯¹è±¡ç±»å‹: {object_type.value}")

                # æ ¹æ®ç±»å‹è°ƒæ•´é…ç½®
                self.adjust_config_for_object_type(object_type)

                # æ ¹æ®å›¾åƒå¤§å°è°ƒæ•´ç‚¹æ•°
                n_points = self.adjust_points_by_size((image.width, image.height))

                # æ£€æµ‹å†…å®¹æ¡†
                content_box = self.sam_core.detect_content_box(image, object_type)

                # åˆ›å»ºåˆå§‹æ©ç 
                initial_mask_roi = self.sam_core.create_initial_mask(image, content_box, object_type)

                # ç”Ÿæˆå‡ ä½•ç‚¹
                geometric_points = self.seg_processor.generate_geometric_points(
                    initial_mask_roi, object_type, n_points=n_points
                )

                # å¤šæç¤ºSAMåˆ†å‰²
                final_mask, score, method = self.seg_processor.multi_prompt_sam_segmentation(
                    image, content_box, initial_mask_roi, geometric_points, object_type
                )

                # V8å¢å¼ºåå¤„ç†
                img_array = np.array(image)
                final_mask = self.post_processor.post_process_mask_enhanced(
                    final_mask, content_box, object_type, method, img_array,
                    fast_mode=self.fast_mode
                )

                # è¯„ä¼°è´¨é‡
                final_quality = self.seg_processor.evaluate_mask_quality(
                    final_mask.cpu().numpy(), object_type
                )
                print(f"ğŸ“Š æœ€ç»ˆè´¨é‡è¯„åˆ†: {final_quality:.3f}")

                # æå–å‰æ™¯
                foreground_only, pixels = self.post_processor.extract_foreground(
                    image, final_mask, content_box
                )
                Image.fromarray(foreground_only, 'RGBA').save(foreground_file)

                # å¯è§†åŒ–ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
                if save_visualization:
                    self.post_processor.visualize_results(
                        image, content_box, initial_mask_roi, geometric_points,
                        final_mask, score, object_type, method, filename, output_path,
                        save_visualization=save_visualization
                    )

                # è®°å½•å¤„ç†æ—¶é—´
                process_time = time.time() - start_time
                self.processing_times.append(process_time)
                print(f"âœ… å®Œæˆ! è€—æ—¶: {process_time:.2f}ç§’")

                # æ¸…ç†å†…å­˜
                del image, img_array, initial_mask_roi, geometric_points, final_mask
                gc.collect()
                if self.device == "cuda":
                    torch.cuda.empty_cache()

                return {
                    'filename': filename,
                    'success': True,
                    'score': score,
                    'quality': final_quality,
                    'object_type': object_type.value,
                    'process_time': process_time
                }

        except TimeoutError as e:
            print(f"â° {filename} å¤„ç†è¶…æ—¶ ({timeout_seconds}ç§’)")
            return {
                'filename': filename,
                'success': False,
                'error': str(e)
            }
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'filename': filename,
                'success': False,
                'error': str(e)
            }
        finally:
            # ç¡®ä¿æ¸…ç†å†…å­˜
            gc.collect()
            if self.device == "cuda":
                torch.cuda.empty_cache()

    def batch_extract_optimized(self, input_dir: str = "input_images",
                                output_dir: str = "batch_output_v8",
                                force_overwrite: bool = True,
                                batch_size: int = 5,
                                timeout_per_image: int = 60,
                                save_visualizations: bool = True) -> List[Dict]:
        """ä¼˜åŒ–çš„æ‰¹é‡å¤„ç† - åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜æº¢å‡º"""
        input_path = Path(input_dir)
        output_path = Path(output_dir)

        if force_overwrite and output_path.exists():
            shutil.rmtree(output_path)
        output_path.mkdir(parents=True, exist_ok=True)

        # è·å–å›¾åƒæ–‡ä»¶åˆ—è¡¨
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        image_files = sorted([p for p in input_path.iterdir()
                              if p.suffix.lower() in image_extensions])

        if not image_files:
            print(f"âŒ åœ¨ '{input_dir}' ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
            return []

        print(f"ğŸ” æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
        print(f"âš™ï¸ æ‰¹å¤„ç†é…ç½®: æ‰¹å¤§å°={batch_size}, è¶…æ—¶={timeout_per_image}ç§’/å›¾")

        results = []
        total_files = len(image_files)

        # åˆ†æ‰¹å¤„ç†
        for batch_start in range(0, total_files, batch_size):
            batch_end = min(batch_start + batch_size, total_files)
            batch_files = image_files[batch_start:batch_end]

            print(f"\n{'=' * 60}")
            print(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_start // batch_size + 1}/{(total_files - 1) // batch_size + 1}")
            print(f"   æ–‡ä»¶ {batch_start + 1}-{batch_end}/{total_files}")
            print(f"{'=' * 60}")

            # å¤„ç†å½“å‰æ‰¹æ¬¡
            for i, image_file in enumerate(batch_files, batch_start + 1):
                print(f"\n[{i}/{total_files}] " + "=" * 50)

                result = self.process_with_extraction(
                    str(image_file),
                    output_path,
                    force_overwrite,
                    timeout_seconds=timeout_per_image,
                    save_visualization=save_visualizations
                )

                if result:
                    results.append(result)

            # æ¯æ‰¹æ¬¡åæ¸…ç†å†…å­˜
            print(f"\nğŸ§¹ æ¸…ç†æ‰¹æ¬¡å†…å­˜...")
            self.post_processor.clear_cache()
            gc.collect()

            if self.device == "cuda":
                torch.cuda.empty_cache()
                # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                if torch.cuda.is_available():
                    allocated = torch.cuda.memory_allocated() / 1024 ** 3
                    reserved = torch.cuda.memory_reserved() / 1024 ** 3
                    print(f"   GPUå†…å­˜: å·²åˆ†é…={allocated:.2f}GB, å·²ä¿ç•™={reserved:.2f}GB")

        # æ‰“å°æœ€ç»ˆæ±‡æ€»
        self._print_summary(results, total_files)

        return results

    def _print_summary(self, results: List[Dict], total_files: int):
        """æ‰“å°å¤„ç†æ±‡æ€»ä¿¡æ¯"""
        successful = sum(1 for r in results if r.get('success'))
        print(f"\n{'=' * 70}")
        print(f"ğŸ“Š æ‰¹å¤„ç†å®Œæˆ: æˆåŠŸ {successful}/{total_files}")

        if successful > 0:
            successful_results = [r for r in results if r.get('success')]

            # æŒ‰ç±»å‹ç»Ÿè®¡
            type_counts = {}
            for r in successful_results:
                obj_type = r.get('object_type', 'unknown')
                type_counts[obj_type] = type_counts.get(obj_type, 0) + 1

            print(f"\nğŸ“ˆ å¯¹è±¡ç±»å‹åˆ†å¸ƒ:")
            for obj_type, count in type_counts.items():
                print(f"   {obj_type}: {count}ä¸ª")

            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            avg_score = np.mean([r['score'] for r in successful_results])
            avg_quality = np.mean([r['quality'] for r in successful_results])
            avg_time = np.mean([r.get('process_time', 0) for r in successful_results if r.get('process_time')])

            print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
            print(f"   å¹³å‡IoUåˆ†æ•°: {avg_score:.3f}")
            print(f"   å¹³å‡è´¨é‡è¯„åˆ†: {avg_quality:.3f}")
            print(f"   å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f}ç§’")

            # ç‰¹åˆ«ç»Ÿè®¡å„ç±»å‹
            for obj_type in type_counts.keys():
                type_results = [r for r in successful_results if r.get('object_type') == obj_type]
                if type_results:
                    type_avg_score = np.mean([r['score'] for r in type_results])
                    type_avg_quality = np.mean([r['quality'] for r in type_results])
                    print(f"\n   {obj_type}ç±»å‹:")
                    print(f"      å¹³å‡IoU: {type_avg_score:.3f}")
                    print(f"      å¹³å‡è´¨é‡: {type_avg_quality:.3f}")

        # å¤±è´¥ç»Ÿè®¡
        failed = [r for r in results if not r.get('success')]
        if failed:
            print(f"\nâš ï¸ å¤±è´¥çš„æ–‡ä»¶ ({len(failed)}ä¸ª):")
            for f in failed[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   - {f['filename']}: {f.get('error', 'æœªçŸ¥é”™è¯¯')}")
            if len(failed) > 5:
                print(f"   ... è¿˜æœ‰ {len(failed) - 5} ä¸ªå¤±è´¥æ–‡ä»¶")

        print(f"{'=' * 70}")

    # ============= å…¼å®¹æ€§æ–¹æ³• =============

    def batch_extract(self, input_dir: str = "input_images",
                      output_dir: str = "batch_output_v8",
                      force_overwrite: bool = True) -> List[Dict]:
        """å…¼å®¹æ€§æ–¹æ³• - è°ƒç”¨ä¼˜åŒ–ç‰ˆæ‰¹å¤„ç†"""
        return self.batch_extract_optimized(
            input_dir=input_dir,
            output_dir=output_dir,
            force_overwrite=force_overwrite,
            batch_size=5,
            timeout_per_image=60,
            save_visualizations=True
        )

    def detect_content_box_enhanced(self, image, object_type):
        """å…¼å®¹æ€§æ–¹æ³•"""
        return self.sam_core.detect_content_box(image, object_type)

    def create_adaptive_mask_for_carving(self, image, content_box, object_type):
        """å…¼å®¹æ€§æ–¹æ³•"""
        return self.sam_core.create_initial_mask(image, content_box, object_type)

    def generate_geometric_points(self, mask, object_type, n_points=120):
        """å…¼å®¹æ€§æ–¹æ³•"""
        return self.seg_processor.generate_geometric_points(mask, object_type, n_points)

    def multi_prompt_sam_segmentation(self, image, content_box, initial_mask,
                                      geometric_points, object_type):
        """å…¼å®¹æ€§æ–¹æ³•"""
        return self.seg_processor.multi_prompt_sam_segmentation(
            image, content_box, initial_mask, geometric_points, object_type
        )

    def post_process_mask_enhanced(self, mask, content_box, object_type, method, image=None):
        """å…¼å®¹æ€§æ–¹æ³•"""
        return self.post_processor.post_process_mask_enhanced(
            mask, content_box, object_type, method, image, fast_mode=self.fast_mode
        )

    def extract_foreground(self, image, mask, content_box=None, crop_to_content=True):
        """å…¼å®¹æ€§æ–¹æ³•"""
        return self.post_processor.extract_foreground(image, mask, content_box, crop_to_content)

    def visualize_results(self, *args, **kwargs):
        """å…¼å®¹æ€§æ–¹æ³•"""
        return self.post_processor.visualize_results(*args, **kwargs)